{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Homework Week 6 (100 pts)\n",
    "\n",
    "In this homework you will use a short corpus of data to build vectors for words and combinations of words, and ultimately build matrices for adjectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables (3 pts)\n",
    "You will need: \n",
    "1. A dictionary in which to store context words (as keys) and their counts (as values)\n",
    "2. A dictionary of dictionaries to store adjectives (as keys), their arguments (as keys to the sub-dictionaries), and counts of adjective-argument occurrences (as values of the sub dictionary). Example: `adjective_counts = {'little':{'mouse':12, 'car':5}, 'big':{'elephant':6, 'car':7}}`. Choose two or more adjectives to learn (`little` and `big` are fine).\n",
    "3. A list to store stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty dicts and lists\n",
    "ctxtwords = dict()\n",
    "adj_count = dict()\n",
    "adj_count['little'] = {}\n",
    "adj_count['big'] = {}\n",
    "stopwords = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords (5 pts)\n",
    "Open the file `stopwords.txt` and read in the stopwords, storing them in the list you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'stopwords.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b8bc4f1e4ddf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# open stopwords and store in a list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stopwords.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'stopwords.txt'"
     ]
    }
   ],
   "source": [
    "# open stopwords and store in a list\n",
    "file = open('stopwords.txt','r')\n",
    "for word in file:\n",
    "    stopwords.append(word.split()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a translation to remove punctuation (2 pts)\n",
    "See this week's practice exercises for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translation table\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation,None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting vocabulary counts (15 points)\n",
    "\n",
    "1. Open the file `story.txt` and read it in one line at a time. (1 pt)\n",
    "2. For each line, use the function `re.split()` to split the line on non-alphanumeric characters. (2 pts)\n",
    "3. For each word in the list of strings, process the word to remove punctuation and put it all in lowercase. Check whether it is already in your dictionary of context words, and if so, increment the count. If not, add it to your dictionary of context words with count 1 (4 pts)\n",
    "4. Also, check whether the word is one of the adjectives you will be learning. If it is, then look at the next word in the line and either increment the count of that word in the dictionary of that adjective, or add the word to the dictionary for that adjective with count 1. (6 pts) \n",
    "5. Note that the adjective may be the last word in the line. In that case, you will have to temporarily store it until the next line is read in. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file and initialize list and \n",
    "story = open('story.txt','r')\n",
    "temp_word = (0,\"\")\n",
    "corpus = []\n",
    "\n",
    "# for every word in the file strip it and clean it\n",
    "for line in story:\n",
    "    split_line = re.split('\\W+',line)\n",
    "    for word in split_line:\n",
    "        clean_word = word.translate(table).lower()\n",
    "        if clean_word != \"\" and clean_word not in stopwords:\n",
    "            corpus.append(clean_word)\n",
    "            \n",
    "# this will be used if the adjective is at the end of the line           \n",
    "for i in range(len(corpus)):\n",
    "    if temp_word[0] == 1 and clean_word != \"\":\n",
    "        if clean_word in adj_count[temp_word[1]]:\n",
    "            adj_count[temp_word[1]][clean_word] += 1\n",
    "        else: \n",
    "            adj_count[temp_word[1]][clean_word] = 1\n",
    "        temp_word = (0,\"\")\n",
    "    # filter out empty strings\n",
    "    if corpus[i] != \"\":\n",
    "        #increment the context words\n",
    "        if corpus[i] in ctxtwords:\n",
    "            ctxtwords[corpus[i]] += 1\n",
    "        else:\n",
    "            ctxtwords[corpus[i]] = 1\n",
    "    \n",
    "    # check if the word is big or little\n",
    "    if corpus[i] in adj_count:\n",
    "        next_clean_word = corpus[i+1]\n",
    "        # check if the next word is not empty\n",
    "        if next_clean_word != \"\":\n",
    "            # add the next word to the adjective count\n",
    "            if next_clean_word in adj_count[corpus[i]]:\n",
    "                adj_count[corpus[i]][next_clean_word] += 1\n",
    "            else:\n",
    "                adj_count[corpus[i]][next_clean_word] = 1\n",
    "        else:\n",
    "            temp_word = (1,clean_word) \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting a list of target words (5 pts)\n",
    "Create a list of context words from the dictionary you built from the corpus. (1 pt)\n",
    "\n",
    "Create a list of target words, encoded as strings, that consists of all the context words and all the adjective-noun combinations you found. (4 pts)\n",
    "\n",
    "Example:\n",
    "\n",
    "`target_word_list = ['car', 'elephant', 'mouse', 'little', 'big', 'little car', 'little mouse', 'big car', 'big mouse']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty lists\n",
    "ctxtwordslist = []\n",
    "target_word_list = []\n",
    "\n",
    "# add all context words to the lists\n",
    "for key in ctxtwords:\n",
    "    ctxtwordslist.append(key)\n",
    "    target_word_list.append(key)\n",
    "\n",
    "# add the adjective-noun combinations to the target word list\n",
    "for word in ctxtwordslist:\n",
    "    if word in adj_count['little']:\n",
    "        target_word_list.append('little '+ word)\n",
    "        target_word_list.append(word)\n",
    "    if word in adj_count['big']:\n",
    "        target_word_list.append('big ' + word)\n",
    "        target_word_list.append(word)\n",
    "        \n",
    "target_word_list = list(set(target_word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting cooccurrence counts (25 points)\n",
    "We cycle through the corpus again and collect cooccurrence counts for each target word (or phrase).\n",
    "\n",
    "1. Set a variable for the width of the window and initialise a list of empty strings of twice the length of the window (1 pt)\n",
    "2. Initialise a matrix of zeros with number of rows equal to the number of target words and phrases, and number of columns equal to the number of context words (1 pt)\n",
    "3. Open the file `story.txt` and read it in one line at a time. (1 pt)\n",
    "4. For each line, split it into a list of strings by breaking on non-alphanumeric characters. (2 pts)\n",
    "5. Process each word in the list to ensure that it is all in lowercase, contains no punctuation, is not a stopword, and is not empty. (2 pts)\n",
    "6. For each word in the list:\n",
    "    - append it to the current window\n",
    "    - check whether the central word in the window is in the set of target words. \n",
    "    - If it is, then iterate over the context words in the current window and increment the entry in the cooccurrence matrix corresponding to (target word, context word) by 1, leaving out the target word itself. (8 pts)\n",
    "    - Check whether the word in front of the target word is one of your adjectives (i.e., the target word is the word in the centre of the context window, and you are checking whether the word in front of it is an adjective)\n",
    "    - If it is, then increment the entry in the coocurrence matrix accordingly (8 pts)\n",
    "7. Discard the first element of the current window (2 pts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters and make the matrix M\n",
    "window_width = 2\n",
    "window = [''] * (window_width*2+1)\n",
    "M = np.zeros((len(target_word_list),len(ctxtwordslist)))\n",
    "\n",
    "# since our corpus is one list, we have to use the if statements because of the end of the list\n",
    "for i in range(len(corpus)-1):\n",
    "    if i == 0:\n",
    "        window[2] = corpus[i]\n",
    "        window[3] = corpus[i+1]\n",
    "        window[4] = corpus[i+2]\n",
    "    elif i == 1:\n",
    "        window[1] = corpus[i-1]\n",
    "        window[2] = corpus[i]\n",
    "        window[3] = corpus[i+1]\n",
    "        window[4] = corpus[i+2]\n",
    "    elif i == len(corpus)-1:\n",
    "        window[0] = corpus[i-2]\n",
    "        window[1] = corpus[i-1]\n",
    "        window[2] = corpus[i]\n",
    "        window[3] = corpus[i+1]\n",
    "        window[4] = \"\"\n",
    "    elif i == len(corpus)-2:\n",
    "        window[0] = corpus[i-2]\n",
    "        window[1] = corpus[i-1]\n",
    "        window[2] = corpus[i]\n",
    "        window[3] = \"\"\n",
    "        window[4] = \"\"\n",
    "    else:\n",
    "        window[0] = corpus[i-2]\n",
    "        window[1] = corpus[i-1]\n",
    "        window[2] = corpus[i]\n",
    "        window[3] = corpus[i+1]\n",
    "        window[4] = corpus[i+2]\n",
    "    # check if the word in the middle of the window is in the target word list\n",
    "    if window[2] in target_word_list: \n",
    "        # get the word its position\n",
    "        pos1 = target_word_list.index(window[2])\n",
    "        # increment the context words from the window\n",
    "        for word in window:\n",
    "            if word != \"\":\n",
    "                pos2 = ctxtwordslist.index(word)\n",
    "                M[pos1][pos2]+=1\n",
    "        # increment the adj_noun combinations as well\n",
    "        if window[1] in adj_count:\n",
    "            pos1 = target_word_list.index(window[1]+' '+window[2])\n",
    "            M[pos1][ctxtwordslist.index(window[1])] += 1\n",
    "            M[pos1][ctxtwordslist.index(window[2])] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating PMI (15 pts)\n",
    "Calculate a version of the PMI that is based only on counts (no logarithm)\n",
    "\n",
    "Recall that the equation is:\n",
    "\n",
    "$$ PMI(t, c) = \\frac{count(t, c)\\times total}{count(t)count(c)}$$\n",
    "\n",
    "where $count(t,c)$ is the number of times that word $t$ cooccurs with word $c$, $count(t)$ is the total number of times that word $t$ occurs in the text, $count(c)$ is the total number of times that word $t$ occurs in the text, and $total$ is the total number of words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty matrix\n",
    "PMI = np.zeros((len(target_word_list),len(ctxtwordslist)))\n",
    "\n",
    "# loop through all rows and column\n",
    "for i in range(len(M)):\n",
    "    for j in range(len(M[i])):\n",
    "        # if there is a \" \" in the word then it is a adj-noun combination\n",
    "        if \" \" in target_word_list[i]:\n",
    "            adj = target_word_list[i].split(\" \")[0]\n",
    "            nn = target_word_list[i].split(\" \")[1]\n",
    "            countw1 = adj_count[adj][nn]\n",
    "        # otherwise just get the count for a single word\n",
    "        else:\n",
    "            countw1 = ctxtwords[target_word_list[i]]\n",
    "        countw2 = ctxtwords[ctxtwordslist[j]]\n",
    "        # calculate PMI\n",
    "        PMI[i][j] = (M[i][j]*len(corpus))/(countw1*countw2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction (10 pts)\n",
    "To reduce the number of dimensions we will use SVD. Recall that the principle behind SVD is that any $m \\times n$ matrix $M$ can be written as \n",
    "\n",
    "$$ M = U \\Sigma V^* $$\n",
    "\n",
    "where $U$ is a $m \\times m$ matrix of orthonormal eigenvectors  for  $MM^*$, $V$ is a $n \\times n$ matrix of orthonormal eigenvectors  for  $M^*M$, and $\\Sigma$ is a diagonal matrix of singular values.\n",
    "\n",
    "In `numpy.linalg.svd`, the singular values are returned in decreasing order. This means that we can select the first $k$ singular values in order to reduce the dimensionality to $k$\n",
    "\n",
    "1. Use the function `numpy.linalg.svd` on the matrix of PMI values to obtain the matrix $U$, the list of singular values $s$ and the matrix $V^*$. (2 pts)\n",
    "2. Plot the singular values and use the shape of the plot to decide to many dimensions `k` to reduce to. State how many dimensions you are reducing to and why. (3pts)\n",
    "3. Shorten the list of singular values to your desired length `k`. Embed the list of singular values in a diagonal matrix $\\Sigma_k$ of shape `(k, k)` (2 pts)\n",
    "4. Now modify $U$ to $U_k$ and $V^*$ to $V^*_k$ so that when you multiply together $U_k \\Sigma_k V^*_k$ you result in a matrix $PMI_k$ of shape $(m, k)$, where $m$ is the number of target words and phrases. I.e., you now have a list of vectors of length $k$, one for each target word or phrase. (3 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute SVD\n",
    "U, S, V = np.linalg.svd(PMI)\n",
    "\n",
    "# show the singular values\n",
    "plt.plot(S)\n",
    "plt.show()\n",
    "\n",
    "# reduce dimensionality\n",
    "S = S[:450]\n",
    "\n",
    "# create the diagonal matrix\n",
    "D = np.diag(S)\n",
    "\n",
    "# adjust U\n",
    "UR = U[:len(target_word_list),:450]\n",
    "\n",
    "MR = UR.dot(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your data (10 pts)\n",
    "\n",
    "For *each* of the adjectives you have chosen, build *two* matrices. One should consist of the vectors for the arguments of the adjective, and the other should consist of the vectors for the adjective-noun combinations.\n",
    "\n",
    "Example: \n",
    "- For `little`, you will have one matrix with the vectors for `mouse` and `car`, and one matrix with the vectors for `little mouse` and `little car`.\n",
    "- For `big` you will have one matrix with the vectors for `elephant` and `car` and one matrix with the vectors for `big elephant` and `big car`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty matrices\n",
    "l_vec1 = np.zeros((450,))\n",
    "l_vec2 = np.zeros((450,))\n",
    "\n",
    "# create the matrices \n",
    "for key in adj_count['little']:\n",
    "    l_vec1 = np.vstack([l_vec1, MR[target_word_list.index(key)]])\n",
    "    l_vec2 = np.vstack([l_vec2, MR[target_word_list.index('little '+key)]])\n",
    "\n",
    "# remove the first empty row\n",
    "l_vec1 = l_vec1[1:47]    \n",
    "l_vec2 = l_vec2[1:47]\n",
    "\n",
    "# initialize empty matrices\n",
    "b_vec1 = np.zeros((450,))\n",
    "b_vec2 = np.zeros((450,))\n",
    "\n",
    "# create the matrices \n",
    "for key in adj_count['big']:\n",
    "    b_vec1 = np.vstack([b_vec1, MR[target_word_list.index(key)]])\n",
    "    b_vec2 = np.vstack([b_vec2, MR[target_word_list.index('big '+key)]])\n",
    "\n",
    "# remove the first empty row\n",
    "b_vec1 = b_vec1[1:47]    \n",
    "b_vec2 = b_vec2[1:47]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning adjective matrices (5 pts)\n",
    "For each of the adjectives you have chosen, infer an adjective matrix from the nouns vectors and the adjective-noun combinations using linear regression (you can use the function `np.linalg.pinv()` to implement the linear regression) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the adjective matrix can be derived by multiplying the pseudo-inverse of the noun-matrix with the noun-adjective matrix n\n",
    "adj1 = np.linalg.pinv(l_vec1).dot(l_vec2)\n",
    "adj2 = np.linalg.pinv(b_vec1).dot(b_vec2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
